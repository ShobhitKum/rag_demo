from langchain.text_splitter import Language
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import LanguageParser
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain.chains import LLMChain
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate

# file_path = './temp/news.txt.rtf'
# loader = TextLoader(file_path)
# document = loader.load()
# #print(document[0])
# text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50, separators=["\n\n", "\n", " ", ""])
# docs = text_splitter.split_documents(document)
#
# embeddings = OllamaEmbeddings()
#
# #db3 = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory="./chroma_db")
# db3 = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
#
# retriever = db3.as_retriever(search_type="mmr",  # Also test "similarity"
#                              search_kwargs={"k": 8})

# retriever = embeddings.as_retriever(
#      search_type="mmr",  # Also test "similarity"
#      search_kwargs={"k": 8},
# )

# result = retriever.get_relevant_documents("Explain IAM policy ")

model = "llama2"


def init_ollama(model_typ):
    # llm = Ollama(model=model_typ,verbose=True,callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))
    llm = ChatOllama(model=model_typ, temperature=0)
    return llm


def init_template():
    summary_template = """
    You are Java programmer. Given the information {information} by client I want you to write :
    1. Java program
    2. Unit test case
    3. pom.xml for this
    """
    prompt_template = PromptTemplate(input_variables=["information"], template=summary_template)
    return prompt_template


#######QA###########
# from langchain import hub
# from langchain.llms import Ollama
# from langchain.callbacks.manager import CallbackManager
# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
# from langchain.prompts import PromptTemplate
# from langchain.chains import RetrievalQA
#
# prompt = PromptTemplate.from_template("Can you share more details on Disney Star and Reliance  deal {context}?")
# prompt.format(context="Can you share more details on Disney Star and Reliance  deal")

# QA_CHAIN_PROMPT = hub.pull("rlm/rag-prompt-llama")

# llm = initOllama(model)


# qa_chain = RetrievalQA.from_chain_type(
#     llm,
#     retriever=db3.as_retriever(),
#     chain_type_kwargs={"prompt": prompt},
# )
# question = "Deal was between which two entities?"
# result2 = qa_chain({"query": question})


information = """ Spring boot project with endpoint as /api/execute making rest api call to http://www.api.google.com/shobhit"""

prompt = init_template()
llm = init_ollama(model)

chain = prompt | llm | StrOutputParser()
# chain = LLMChain(llm=init_ollama(model), prompt=init_template())
res = chain.invoke(input={"information": information})
print(res)
